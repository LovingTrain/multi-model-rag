{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clip encode mode\n",
    "CLIP 模型既包含了image encoder，又包含了text encoder，通过对比损失，让它们在空间距离相近\n",
    "encode model，网络差的提前下载好。\n",
    "\n",
    "* ViT-L/14 1.7G\n",
    "* ViT-B/32 600M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "- coco 20G左右\n",
    "- Flickr30k 2G\n",
    "- LAION-5B 240T 太大了 需要第三方库 img2dataset 去构建向量索引库\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "数据集下载后转为下面的格式\n",
    "/path/to/your/dataset/\n",
    "├── image1.jpg\n",
    "├── image1.txt (可选的图片描述)\n",
    "├── image2.png\n",
    "├── image2.txt\n",
    "└── ...\n",
    "''' \n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm  # 用于显示漂亮的进度条\n",
    "\n",
    "\n",
    "def convert_dataset_to_folder_format(\n",
    "    annotations_file,\n",
    "    images_dir,\n",
    "    output_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    将包含图片和JSON描述文件的数据集转换为 \"图片-文本对\" 文件夹格式。\n",
    "\n",
    "    Args:\n",
    "        annotations_file (str): 指向包含描述信息的 JSON 文件的路径。\n",
    "        images_dir (str): 存放所有原始图片文件的目录。\n",
    "        output_dir (str): 用于存放转换后数据的输出目录。\n",
    "    \"\"\"\n",
    "    # 确保输出目录存在\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # 加载描述文件\n",
    "    with open(annotations_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load()\n",
    "\n",
    "    # 假设 JSON 结构是 {'annotations': [{'image_id': ..., 'caption': ...}], 'images': [{'id': ..., 'file_name': ...}]}\n",
    "    # 这是 MS COCO 数据集的典型结构，很多其他数据集也类似\n",
    "\n",
    "    # 创建一个从 image_id 到 file_name 的映射\n",
    "    image_id_to_filename = {img['id']: img['file_name']\n",
    "                            for img in data['images']}\n",
    "\n",
    "    print(f\"开始转换 {len(data['annotations'])} 条描述...\")\n",
    "\n",
    "    # 遍历所有描述\n",
    "    for annotation in tqdm(data['annotations']):\n",
    "        image_id = annotation['image_id']\n",
    "        caption = annotation['caption']\n",
    "\n",
    "        if image_id in image_id_to_filename:\n",
    "            # 获取原始文件名和路径\n",
    "            original_filename = image_id_to_filename[image_id]\n",
    "            source_image_path = os.path.join(images_dir, original_filename)\n",
    "\n",
    "            # 检查原始图片是否存在\n",
    "            if not os.path.exists(source_image_path):\n",
    "                continue\n",
    "\n",
    "            # 定义输出文件的基本名 (不含扩展名)\n",
    "            base_name = os.path.splitext(original_filename)[0]\n",
    "\n",
    "            # 定义输出文本文件和图片文件的路径\n",
    "            output_txt_path = os.path.join(output_dir, f\"{base_name}.txt\")\n",
    "            output_img_path = os.path.join(output_dir, original_filename)\n",
    "\n",
    "            # 写入文本描述\n",
    "            # 注意：一个图片可能有多个描述，这里选择覆盖写入或追加写入\n",
    "            # 此处使用追加模式(a)，允许多个描述保存在一个文件中\n",
    "            with open(output_txt_path, 'a', encoding='utf-8') as txt_file:\n",
    "                txt_file.write(caption.strip() + '\\n')\n",
    "\n",
    "            # 复制图片文件 (如果尚未复制)\n",
    "            if not os.path.exists(output_img_path):\n",
    "                import shutil\n",
    "                shutil.copy(source_image_path, output_img_path)\n",
    "\n",
    "    print(\"转换完成！\")\n",
    "\n",
    "# --- 使用示例 ---\n",
    "# 假设你已经下载了 MS COCO 数据集\n",
    "# annotations_file = '/path/to/coco/annotations/captions_train2017.json'\n",
    "# images_dir = '/path/to/coco/train2017'\n",
    "# output_dir = '/path/to/your/dataset_formatted'\n",
    "\n",
    "# convert_dataset_to_folder_format(annotations_file, images_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建索引\n",
    "import torch\n",
    "import clip\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def process_embeddings():\n",
    "    IMAGE_FOLDER = \"path/to/your/image_folder\"  # coco下载数据集的位置\n",
    "    OUTPUT_FOLDER = \"path/to/your/new_gpu_index\"  # 输出索引和元数据的文件夹\n",
    "    CLIP_MODEL_PATH = \"ViT-B/32\"               # 提前下载好的 CLIP 模型\n",
    "    BATCH_SIZE = 256                           # 批处理大小，根据您的 GPU 显存调整\n",
    "\n",
    "    # --- 1. 初始化模型和设备 ---\n",
    "    print(\">>> 步骤 1: 初始化模型和设备...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cpu\":\n",
    "        print(\"警告：未检测到 CUDA，将使用 CPU。这会非常慢。\")\n",
    "\n",
    "    # 加载 CLIP 模型和预处理器\n",
    "    model, preprocess = clip.load(CLIP_MODEL_PATH, device=device, jit=False)\n",
    "    print(f\"模型 {CLIP_MODEL_PATH} 已加载到 {device}。\")\n",
    "\n",
    "\n",
    "    print(\"\\n>>> 步骤 2: 生成图片 Embeddings...\")\n",
    "    if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "    image_paths = list(glob.glob(f\"{IMAGE_FOLDER}/**/*.jpg\", recursive=True)) + \\\n",
    "        list(glob.glob(f\"{IMAGE_FOLDER}/**/*.png\", recursive=True))\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_metadata = []\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), BATCH_SIZE), desc=\"处理图片批次\"):\n",
    "        batch_paths = image_paths[i:i + BATCH_SIZE]\n",
    "        batch_images = []\n",
    "\n",
    "        # 预处理图片\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                image = preprocess(Image.open(path)).unsqueeze(0)\n",
    "                batch_images.append(image)\n",
    "            except Exception as e:\n",
    "                print(f\"警告：无法处理图片 {path}，已跳过。错误: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not batch_images:\n",
    "            continue\n",
    "\n",
    "        batch_tensor = torch.cat(batch_images).to(device)\n",
    "\n",
    "        # 使用 CLIP 模型进行推理\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(batch_tensor)\n",
    "\n",
    "        # L2 归一化，为余弦相似度搜索做准备\n",
    "        faiss.normalize_L2(image_features)\n",
    "\n",
    "        # 将向量移回 CPU 并添加到列表中\n",
    "        all_embeddings.append(image_features.cpu().numpy())\n",
    "        all_metadata.extend(batch_paths)  # 记录对应的文件路径\n",
    "\n",
    "    # 将所有批次的向量合并成一个大的 numpy 数组\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    all_metadata  = np.vstack(all_metadata)\n",
    "\n",
    "    np.save(OUTPUT_FOLDER+\"/all_embeddings.npy\", all_embeddings)\n",
    "    np.save(OUTPUT_FOLDER+\"/all_metadata.npy\", all_metadata)\n",
    "\n",
    "    print(\"embedding finished ~ ~\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissSearcher():\n",
    "    def __init__(self, embedding_path,metadata_path) -> None:\n",
    "        \n",
    "        all_embeddings = np.load(embedding_path)\n",
    "        self.metadata_df = pd.read_feather(metadata_path)\n",
    "        \n",
    "        self.all_embeddings = all_embeddings\n",
    "        N = all_embeddings.shape[0]\n",
    "        self.split_idx = int(0.7 * N)\n",
    "    # IVF 索引,有聚类\n",
    "    def init_cpu_index(self):\n",
    "        d = self.all_embeddings.shape[1]\n",
    "        nlist = int(4 * np.sqrt(len(self.all_embeddings)))\n",
    "        quantizer = faiss.IndexFlatL2(d) \n",
    "        cpu_index_ivfflat = faiss.IndexIVFFlat(\n",
    "            quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "        # 需要先 train,训练索引 无监督的聚类，将数据划分成若干个分区。\n",
    "        cpu_index_ivfflat.train(self.all_embeddings)\n",
    "        cpu_index_ivfflat.add(self.all_embeddings)\n",
    "        cpu_index_ivfflat.nprobe = 16\n",
    "        return cpu_index_ivfflat\n",
    "    # IVF 索引,有聚类\n",
    "    def init_gpu_index(self):\n",
    "        \n",
    "        cpu_index_ivfflat = self.init_cpu_index()\n",
    "        # 准备 GPU 资源 (一次性)\n",
    "        res = faiss.StandardGpuResources()\n",
    "\n",
    "        gpu_search_index = faiss.index_cpu_to_gpu(\n",
    "            res, 0, cpu_index_ivfflat)\n",
    "        gpu_search_index.nprobe = 16\n",
    "\n",
    "        return gpu_search_index\n",
    "    #  Flat 索引——没有聚类，也没有 nprobe 的概念\n",
    "    def init_hybrid_index(self):\n",
    "\n",
    "        \n",
    "        data_cpu = self.all_embeddings[:self.split_idx, :]    # 前 70%\n",
    "        data_gpu = self.all_embeddings[self.split_idx:, :]    # 后 30%\n",
    "        \n",
    "        d = data_cpu.shape[1]\n",
    "        # \n",
    "        hybrid_cpu_index = faiss.IndexFlatL2(d)\n",
    "        hybrid_cpu_index.add(data_cpu)\n",
    "\n",
    "        res = faiss.StandardGpuResources()\n",
    "\n",
    "\n",
    "        gpu_index_cpu_temp = faiss.IndexFlatL2(d)\n",
    "        gpu_index_cpu_temp.add(data_gpu)\n",
    "\n",
    "        hybrid_gpu_index = faiss.index_cpu_to_gpu(res, 0, gpu_index_cpu_temp)\n",
    "\n",
    "\n",
    "        shard_index = faiss.IndexShards(d,\n",
    "                                        threaded=False,     # 是否多线程\n",
    "                                        successive_ids=True)  # 是否按顺序合并ID\n",
    "        shard_index.add_shard(hybrid_cpu_index)\n",
    "        shard_index.add_shard(hybrid_gpu_index)\n",
    "\n",
    "        return shard_index\n",
    "\n",
    "    def init_hybrid_index_ivf(self):\n",
    "        \"\"\"\n",
    "        [优化 4] 创建一个真正混合的 IVF 索引，CPU 和 GPU 上都是 IVF。\n",
    "        [优化 1] 使用 successive_ids=True 来解决索引管理问题。\n",
    "        \"\"\"\n",
    "\n",
    "        res = faiss.StandardGpuResources()\n",
    "        d = self.all_embeddings.shape[1]\n",
    "        nlist = int(4 * np.sqrt(len(self.all_embeddings)))\n",
    "        # 1. 创建两个独立的 CPU IVF 索引\n",
    "        quantizer = faiss.IndexFlatL2(d)\n",
    "\n",
    "        # CPU 部分的索引\n",
    "        cpu_shard = faiss.IndexIVFFlat(\n",
    "            quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "        # GPU 部分的索引（临时在 CPU 上创建）\n",
    "        gpu_shard_cpu_version = faiss.IndexIVFFlat(\n",
    "            quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "        # 2. 创建一个分片索引，并设置 ID 自动连续\n",
    "        hybrid_index = faiss.IndexShards(\n",
    "            d, threaded=False, successive_ids=True)\n",
    "\n",
    "        # 3. 训练和添加数据\n",
    "        split_idx = int(0.7 * len(self.all_embeddings))\n",
    "        data_cpu = self.all_embeddings[:split_idx]\n",
    "        data_gpu = self.all_embeddings[split_idx:]\n",
    "\n",
    "        # 训练需要所有数据来获得一个好的全局聚类中心\n",
    "        print(\"使用全部数据训练共享的量化器...\")\n",
    "        cpu_shard.train(self.all_embeddings)\n",
    "\n",
    "        # 将训练好的量化器赋给 GPU 分片\n",
    "        gpu_shard_cpu_version.quantizer = cpu_shard.quantizer\n",
    "        gpu_shard_cpu_version.is_trained = True\n",
    "\n",
    "        print(\"向 CPU 分片添加数据...\")\n",
    "        cpu_shard.add(data_cpu)\n",
    "        cpu_shard.nprobe = 16\n",
    "\n",
    "        print(\"向 GPU 分片添加数据...\")\n",
    "        gpu_shard_cpu_version.add(data_gpu)\n",
    "\n",
    "        # 将 GPU 分片从 CPU 转换到 GPU\n",
    "        gpu_shard = faiss.index_cpu_to_gpu(res, 0, gpu_shard_cpu_version)\n",
    "        gpu_shard.nprobe = 16\n",
    "\n",
    "        # 4. 将两个分片添加到主索引中\n",
    "        hybrid_index.add_shard(cpu_shard)\n",
    "        hybrid_index.add_shard(gpu_shard)\n",
    "\n",
    "        return hybrid_index\n",
    "\n",
    "\n",
    "    def cpu_search(self,cpu_search_index,query_vector, k=5):\n",
    "\n",
    "        distances, indices = cpu_search_index.search(query_vector, k)\n",
    "        self.print_search_results(distances, indices)\n",
    "        # return distances, indices\n",
    "\n",
    "    def gpu_search(self,gpu_search_index, query_vector, k=5):\n",
    "\n",
    "        distances, indices = gpu_search_index.search(query_vector, k)\n",
    "        self.print_search_results(distances, indices)\n",
    "        # return distances, indices\n",
    "\n",
    "    def hybrid_search(self,shard_index, query_vector, k=5):\n",
    "        \n",
    "        distances, indices = shard_index.search(query_vector, k)\n",
    "        self.print_search_results(distances, indices)\n",
    "        return distances, indices    \n",
    "\n",
    "\n",
    "    def print_search_results(self,distances, indices,k=5):\n",
    "        for i in range(k):\n",
    "            idx = indices[0][i]\n",
    "            filepath = self.metadata_df.iloc[idx]['filepath']\n",
    "            similarity = distances[0][i]\n",
    "            print(\n",
    "                f\"  - 相似度: {similarity:.4f}, 路径: {os.path.basename(filepath)}\")\n",
    "    \n",
    "    def llm_generation(self, query_text, retrieval_results,k=5):\n",
    "        '''\n",
    "        带完善用Qwen generation,API ,还不行，图片没发送进去。\n",
    "        '''\n",
    "        distances, indices = retrieval_results\n",
    "        # 2. 从检索结果中提取上下文信息\n",
    "        context_items = []\n",
    "        for i in range(min(k, len(indices[0]))):\n",
    "            idx = indices[0][i]\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            filepath = self.metadata_df.iloc[idx]['filepath']\n",
    "            similarity = distances[0][i]\n",
    "            # 我们只提取文件名，因为完整路径可能太长且包含不相关信息\n",
    "            filename = os.path.basename(filepath)\n",
    "            context_items.append(f\"- 文件名: {filename} (相似度: {similarity:.4f})\")\n",
    "        \n",
    "        context_str = \"\\n\".join(context_items)\n",
    "        # 3. 精心构建 Prompt\n",
    "        # System Prompt 定义了模型的角色和行为准则\n",
    "        system_prompt = \"你是一个智能的图片内容分析助手。你的任务是基于用户的问题和系统提供的相关图片文件列表，对这些图片的主题、内容或共同点进行推断和总结，并用自然、流畅的中文回答用户。\"\n",
    "        \n",
    "        # User Prompt 包含了用户的请求和我们提供的上下文\n",
    "        user_prompt = f\"\"\"\n",
    "                    用户的原始问题是：\"{query_text}\"\n",
    "\n",
    "                    根据这个问题，我为您检索到了以下最相关的图片文件列表：\n",
    "                    {context_str}\n",
    "\n",
    "                    请注意，你无法直接看到图片内容，但你可以根据文件名、路径和相似度得分进行高质量的推断。\n",
    "                    请综合分析这些信息，回答用户的问题。请不要直接罗列文件名，而是对内容进行归纳总结。\n",
    "                    \"\"\"\n",
    "        from dashscope import Generation\n",
    "        # 4. 调用 DashScope API\n",
    "        response = Generation.call(\n",
    "            model='qwen-turbo',  # 使用 qwen-turbo 模型，性价比高\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=user_prompt\n",
    "        )\n",
    "\n",
    "        return response.output.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the rag breakpoint\n",
    "from PIL import Image\n",
    "if __name__ == '__main__':\n",
    "    process_embeddings()\n",
    "\n",
    "    EMBEDDING_FILE = \"path/to/your/embeddings.npy\"\n",
    "    METADATA_FILE = \"path/to/your/metadata.arrow\"\n",
    "\n",
    "    searcher = FaissSearcher(EMBEDDING_FILE, METADATA_FILE)\n",
    "    cpu_index = searcher.init_cpu_ivf_index()\n",
    "    gpu_index = searcher.init_gpu_ivf_index()\n",
    "\n",
    "    model_path = \"xx/clip/clip-vit-large-patch14.pt\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    embedding_model, preprocess = clip.load(model_path, device=device, jit=False)\n",
    "    \n",
    "    query_text_1 = \"a dog playing on the beach\"\n",
    "    tokenized_text = clip.tokenize([query_text_1]).to(device)\n",
    "    query_img_2 =  preprocess(Image.open(\"your_image.jpg\")).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_vector_1 = embedding_model.encode_text(tokenized_text)\n",
    "            query_vector_2 = embedding_model.encode_image(query_img_2)\n",
    "    searcher.cpu_search(cpu_index,query_vector_1)\n",
    "    searcher.gpu_search(gpu_index,query_vector_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949510482b8eeb0db0de64008f8e13a21a90dd06afd7a8e21dd837eed22f74f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
